# 2023.2: Properties and Testing

###### tags: `Tag(sp23)`

## _Why_ Logic for Systems?

_Formal methods_ are a broad collection of techniques for modeling, reasoning about, verifying, and understanding software, hardware, and other kinds of systems. 

Note that when we say "systems" in this class, we don't necessarily mean the kind of "systems" you see in CSCI 0300 and 0330. Sure, you can apply the techniques of 1710 to material from those classes, but you can also apply it to user interfaces, type systems in programming, hardware, version control systems like Git, web security, cryptographic protocols, robotics, and much more---just within Computer Science. *This is not a "systems" course, and there's no more than minor overlap with any other course at Brown.*

<!-- We'll focus on a field called _lightweight formal methods_. Jeanette Wing and Daniel Jackson wrote a short article on\ this in the 90's, which you can find [online](http://www.cs.cmu.edu/~wing/publications/JacksonWing96.pdf).-->

1710 will teach you some concrete formal methods. It will also prepare you to engage with others, if you're interested in doing so. Some industrial examples I'm fond of include:
* [Amazon Web Services' Zelkova](https://aws.amazon.com/blogs/security/protect-sensitive-data-in-the-cloud-with-automated-reasoning-zelkova/), which helps administrators author better security policies for their services;
* [Microsoft's static driver verifier](https://www.microsoft.com/en-us/research/publication/thorough-static-analysis-of-device-drivers/), which helps increase the reliability of low-level device drivers in Windows;
* [MongoDB's work on modeling replication](https://github.com/visualzhou/mongo-repl-tla), which found a real bug in their code. Quoting the linked page: "We've never encountered this issue in testing or in the field and only found it by reasoning about the edge cases. This shows writing and model checking ... specs is an excellent alternative way to find and verify edge cases." (Ellipsis mine.)

We can find applications for FM outside Computer Science too---even the law. [Here's an article](https://roundtablelaw.medium.com/utterly-unpersuasive-formal-methods-and-law-bb8ecf048374) about the value of modeling _legal concepts_ to find loopholes in the law. This is the sort of FM we'll be learning how to do in 1710.

[This Github repository](https://github.com/ligurio/practical-fm) keeps a (reasonably up to date, but not exhaustive!) list of other industrial applications of formal methods. 

### (Exercise) For Next Time

Can you find one or two of these applications that especially interest you? Alternatively, think about other kinds of "system" you interact with regularly, or have learned about. What would you like to understand better about those systems? (A prior-year final project modeled the rules of baseball, and we all learned something about the game in the process.)

## Logistics

The first assignment goes out today: it's a homework, in Python. If you're not familiar with Python, you should check out our optional Python lab. The homework uses a library called Hypothesis, which we'll see some of in lecture today.

Next Wednesday's lab will use ChatGPT. It's the only part of the course where using ChatGPT is allowed; instead, we'll be providing you access to GPT-3 via a VSCode extension. We'll send more information soon, including the signup form.

## Where are we going?

Most of us have learned how to write test cases. Given an input, here's the output to expect. We talked a bit last time about how tests aren't always good enough: they carry our biases, they can't cover an infinite input space, etc. But even more, they're not always adequate carriers of intent: if I write `assert median([1,2,3]) == 2`, what exactly is the behavior of the system I'm trying to confirm? Surely I'm not writing the test because I care specifically about `[1,2,3]` but not about `[3,4,5]` in the same way? Maybe there was some broader aspect, some _property_ of median I cared about when I wrote that test. What do you think it was?

<details>
<summary>Think, then click!</summary>
There might be many things! One particular idea is that, if the input list has odd length, the median needs to be an element of the list. 
</details>
</br>

There isn't always an easy-to-extract property for every test. But this idea---of encoding _goals_ instead of specific behaviors, forces us to start thinking critically about _what we want_ from a system. It's only a short hop from there to some of the real applications above.

## A New Kind of Testing

### Important Note

1710 is built to be inclusive. We've got first year students and grad students in the same classroom. Depending on which courses you've taken in the past, you might seen some of the ideas we'll discover today (or on other days) already. If that's the case, _avoid giving spoilers_, and don't dismiss the value of review.

### Cheapest Paths

Consider the problem of finding cheapest paths in a weighted graph. There are quite a few algorithms you might use: Dijkstra, Bellman-Ford, even a plain breadth-first search for an unweighted graph. You might have implemented one of these for another class! 

The problem statement seems simple: take a graph $GRAPH$ and two vertex names $V1$ and $V2$ as input. Produce the cheapest path from $V1$ to $V2$ in $GRAPH$. But it turns out that this problem hides a lurking issue.

Find the cheapest path from vertex $G$ to vertex $E$ on the graph below.

![](https://i.imgur.com/CT7MSgl.jpg)

<details>
<summary>Think, then click!</summary>
The path is G to A to B to E.    
    
Great! We have the answer. Now we can go and add a test case for with that graph as input and (G, A, B, E) as the output. 
    
Wait -- you found a different path? G to D to B to E?
    
And another path? G to H to F to E?
</details>

If we add a traditional test case corresponding to _one_ of the correct answers, our test suite will falsely raise alarms for correct implementations that happen to find different answers. In short, we'll be over-fitting our tests to @italic{one specific implementation}: ours. But there's a fix. Maybe instead of writing:

`shortest(GRAPH, G, E) == [(G, A), (A, B), (B, E)]`

we write:

```
shortest(GRAPH, G, E) == [(G, A), (A, B), (B, E)] or
shortest(GRAPH, G, E) == [(G, D), (D, B), (B, E)] or
shortest(GRAPH, G, E) == [(G, H), (H, F), (F, E)]
```

What's wrong with the "big or" strategy? Can you think of a graph where it'd be unwise to try to do this?

<details>
    <summary>Think, then click!</summary>

There are at least two problems. First, we might have missed some possible solutions, which is quite easy to do; the first time Tim was preparing these notes, he missed the third path above, and students pointed it out in class. Second, there might be an unmanageable number of equally correct solutions. (The most pathological case Tim could think of was a graph with all possible edges present, all of which have weight zero).
    
</details>



This problem -- multiple correct answers -- occurs in every part of Computer Science once you're looking for it. Most graph problems exhibit it. Worse, so do most optimization problems. Unique solutions are convenient, but the universe isn't built for our convenience. 

What's the solution? If _test cases_ won't work, is there an alternative? (Hint: instead of defining correctness bottom-up, by small test cases, think top-down: can we say what it __means__ for an implementation to be correct, at a high level?)

<details>
<summary>Think, then click!</summary>
In the cheapest-path case, we can notice that the costs of all cheapest paths are the same. This enables us to write:

`cost(cheapest(GRAPH, G, E)) = 11`

which is now robust against multiple implementations of `cheapest`.
    
</details>


This might be something you were taught to do when implementing cheapest-path algorithms, or it might be something you did on your own, unconsciously. We're not going to stop there, however.

Notice that we just did something subtle and interesting. Even if there are a billion cheapest paths between two vertices in the input graph, they all have that same, minimal length. Like the hint above hinted at, our testing strategy has just evolved past naming _specific_ output values to checking broader _properties_ of output values.

Similarly, we can move past specific inputs and write a function `is_valid` that takes an arbitrary `input, output` pair and returns true if and only if the output is a valid solution for the input. Just pipe in a bunch of inputs, and the function will try them all. You can apply this strategy to most any problem, in any programming language. (For your homework this week, you'll be using Python.)

Let's be more careful, though. Is there something _else_ that `cheapest` needs to guarantee for that input, beyond finding a path with the same cost as our solution?

<details>
<summary>Think, then click!</summary>
We also need to confirm that the path returned by `cheapest` is indeed a path in the graph! This is a _separate_ idea, even though, depending on how the paths are returned, computing its cost might involve walking the graph. 
</details>



In fact, there are still more things we should check. Here's a sketch of an `is_valid` function for cheapest path:

```
isValid : input: (graph, vertex, vertex), output: list(vertex) -> bool
  returns true IFF:
    (1) cost(output) is cost(trustedImplementation(output))
    (2) every vertex in output is in input's graph
    (3) every step in output is an edge in input
    ...
}
```

This style of testing is called Property-Based Testing (PBT). When we're using a trusted implementation, or some other artifact that helps us evaluate the output, it will sometimes be called Model-Based Testing (MBT).

There are a few questions, though...

**Question:** Can we really trust the "trusted" implementation?

No, not completely. But often, questions of correctness are really about the transfer of confidence: my old, slow implementation has worked for a couple of years now, and it's probably mostly right. I don't trust my new, optimized implementation at all: maybe it uses an obscure data structure, or a language I'm not familiar with, or maybe I don't even have access to the source code at all. 

And anyway, often we don't need recourse to any trusted model; we can just phrase the properties directly. 

**Exercise:** What if we don't have a trusted implementation?

You can use this approach whenever you can write a function that checks the correctness of a given output. It doesn't need to use an existing implementation (it's just easier to talk about that way). In the next example we won't use a trusted implementation at all!

**Question:** Where do the inputs come from?

Great question! Some we will manually create based on our own cleverness and understanding of the problem. Others, we'll generate randomly.

Random inputs are used for many purposes in software engineering: "fuzz testing", for instance, creates vast quantities of random inputs in an attempt to find crashes and other serious errors. We'll use that same idea here, except that our notion of correctness is usually a bit more nuanced.

Concretely:

![](https://i.imgur.com/gCGDK6m.jpg)

It's important to note that some creativity is still involved here: you need to come up with an `is_valid` function (the "property"), and you'll almost always want to create some hand-crafted inputs (don't trust a random generator to find the subtle corner cases you already know about!) The strength of this approach lies in its resilience against problems with multiple correct answers, and in its ability to _mine for bugs while you sleep_. Did your random testing find a bug? Fix it, and then add that input to your list of regression tests. Rinse, repeat.

If we were still thinking in terms of traditional test cases, this would make no sense: where would the outputs come from? Instead, we've created a testing system where concrete outputs aren't something we need to provide. Instead, we check whether the program under test produces _any valid output_.

## Hypothesis

There are PBT libraries for most every popular language. For your homework, you'll be using a library for Python called Hypothesis. I want to spend the rest of class stepping you through using the library. Let's test a function in Python itself: the `median` function in the `statistics` library. What are some important properties of `median`?

Now let's use Hypothesis to test at least one of those properties. We'll start with this template:

```python
from hypothesis import given, settings
from hypothesis.strategies import integers, lists
from statistics import median

@given(lists(integers()))
@settings(max_examples=500)
def test_python_median(l):    
    pass

if __name__ == "__main__":
    test_python_median()

```

After some back and forth (see the video recording) we might end up somewhere like this:

```python
def test_python_median(input_list):
    output_median = median(input_list)
    print(f'{input_list} -> {output_median}')
    if len(input_list) % 2 == 1:
        assert output_median in input_list
    
    # Question 1: What's going wrong? The _property_ seems reasonable...
    lower_or_eq =  [val for val in input_list if val <= output_median]
    higher_or_eq = [val for val in input_list if val >= output_median]
    assert len(lower_or_eq) >= len(input_list) // 2    # floor
    assert len(higher_or_eq) >= len(input_list) // 2   # floor
    # Question 2: Is this enough? :-)
```

There's more to do, but hopefully this gives you a reasonably nice starting point.
